{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Baseline without RAG\n",
    "\n",
    "This notebook is a baseline for the QA task without the RAG model. For a fair comparison, we choose the same backbone model as the one in the RAG pipeline: the `meta/llama3.1-8b-Instruct` model. We also adopt the same data type (fp16) and the same config for setting up the tokenizer. We use the same prompt format as the one in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d7e8c342664811ae2977b14cd8d022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "generation_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: load qa annotation test set\n",
    "import pandas as pd\n",
    "\n",
    "qa_df = pd.read_csv(\"../data/crawled_text_datas/QA_pairs_1.csv\")\n",
    "\n",
    "# doc_ids = qa_df[\"Doc_id\"].tolist()\n",
    "questions = qa_df[\"Question\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Bạn là một trợ lý chuyên gia trả lời các câu hỏi thực tế về nhiều khía cạnh của Đại học quốc gia Hà Nội, bao gồm lịch sử, chính sách, văn hóa, sự kiện và nhiều hơn nữa.\n",
    "Nếu bạn không biết câu trả lời, hãy nói \"Tôi không biết.\"\n",
    "\n",
    "Hướng dẫn quan trọng:\n",
    "- Trả lời ngắn gọn, không lặp lại câu hỏi.\n",
    "- KHÔNG dùng câu hoàn chỉnh. Chỉ cung cấp từ, tên, ngày tháng hoặc cụm từ trực tiếp trả lời câu hỏi. Ví dụ, với câu hỏi \"Đại học Quốc gia Hà Nội được thành lập năm nào?\", bạn chỉ trả lời \"1993\".\n",
    "\n",
    "Ví dụ:\n",
    "Q: Đại học Quốc Gia Hà Nội được thành lập năm nào? A: 1993\n",
    "Q: Hiệu trưởng đầu tiên của trường đại học công nghệ là ai? A: Nguyễn Văn Hiệu\n",
    "\n",
    "Câu hỏi: {question} \\n\\n\n",
    "\n",
    "Trả lời:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/574 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  2%|▏         | 10/574 [00:02<02:15,  4.17it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 574/574 [02:38<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# use the template the generate the answers\n",
    "from tqdm import tqdm\n",
    "generated_answers = []\n",
    "for question in tqdm(questions):\n",
    "    full_prompt = template.format(question=question)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": full_prompt},\n",
    "        ]\n",
    "    output = generation_pipe(messages, max_new_tokens=50)\n",
    "    generated_answers.append(output[0][\"generated_text\"][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all columns to a csv file\n",
    "# results_df = pd.DataFrame({\n",
    "#         \"Doc_id\": doc_ids,\n",
    "#         \"Question\": questions,\n",
    "#         \"Reference_Answers\": answers,\n",
    "#         \"Generated_Answer\": generated_answers,\n",
    "#     })\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "        \"Question\": questions,\n",
    "        \"Generated_Answer\": generated_answers,\n",
    "    })\n",
    "\n",
    "# save the results to a csv file\n",
    "results_df.to_csv(\"../output/baseline_no_rag.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
