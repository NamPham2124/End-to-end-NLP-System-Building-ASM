{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Baseline without RAG\n",
    "\n",
    "This notebook is a baseline for the QA task without the RAG model. For a fair comparison, we choose the same backbone model as the one in the RAG pipeline: the `meta/llama3.1-8b-Instruct` model. We also adopt the same data type (fp16) and the same config for setting up the tokenizer. We use the same prompt format as the one in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d7e8c342664811ae2977b14cd8d022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "generation_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: load qa annotation test set\n",
    "import pandas as pd\n",
    "# qa_df = pd.read_csv(\"../data/annotated/QA_pairs_1.csv\")\n",
    "qa_df = pd.read_csv(\"../data/test/test_questions.csv\")\n",
    "\n",
    "# doc_ids = qa_df[\"Doc_id\"].tolist()\n",
    "questions = qa_df[\"Question\"].tolist()\n",
    "# answers = qa_df[\"Reference_Answers\"].tolist()\n",
    "\n",
    "# # random sample 10 qa pairs\n",
    "# import random\n",
    "# sample_size = 10\n",
    "# random.seed(747)\n",
    "# sample_indices = random.sample(range(len(questions)), sample_size)\n",
    "# sample_doc_ids = [doc_ids[i] for i in sample_indices]\n",
    "# sample_questions = [questions[i] for i in sample_indices]\n",
    "# sample_answers = [answers[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an expert assistant answering factual questions about various aspects of Pittsburgh or Carnegie Mellon University (CMU), including history, policy, culture, events, and more. \n",
    "If you do not know the answer, just say \"I don't know.\"\n",
    "\n",
    "Important Instructions:\n",
    "- Answer concisely without repeating the question.\n",
    "- Do **not** use complete sentences. Provide only the word, name, date, or phrase that directly answers the question. For example, given the question \"When was Carnegie Mellon University founded?\", you should only answer \"1900\".\n",
    "\n",
    "Examples:\n",
    "Question: Who is Pittsburgh named after? \n",
    "Answer: William Pitt\n",
    "Question: What famous machine learning venue had its first conference in Pittsburgh in 1980? \n",
    "Answer: ICML\n",
    "Question: What musical artist is performing at PPG Arena on October 13? \n",
    "Answer: Billie Eilish\n",
    "\n",
    "Question: {question} \\n\\n\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/574 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  2%|▏         | 10/574 [00:02<02:15,  4.17it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 574/574 [02:38<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# use the template the generate the answers\n",
    "from tqdm import tqdm\n",
    "generated_answers = []\n",
    "for question in tqdm(questions):\n",
    "    full_prompt = template.format(question=question)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": full_prompt},\n",
    "        ]\n",
    "    output = generation_pipe(messages, max_new_tokens=50)\n",
    "    generated_answers.append(output[0][\"generated_text\"][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all columns to a csv file\n",
    "# results_df = pd.DataFrame({\n",
    "#         \"Doc_id\": doc_ids,\n",
    "#         \"Question\": questions,\n",
    "#         \"Reference_Answers\": answers,\n",
    "#         \"Generated_Answer\": generated_answers,\n",
    "#     })\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "        \"Question\": questions,\n",
    "        \"Generated_Answer\": generated_answers,\n",
    "    })\n",
    "\n",
    "# save the results to a csv file\n",
    "results_df.to_csv(\"../output/submission/closebook_baseline.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
